import time
import random
import re
from typing import List, Set, Optional, Literal
import datetime

import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st
import plotly.express as px  # ì‹œê³„ì—´ ì°¨íŠ¸ë¥¼ ì˜ˆì˜ê²Œ ê·¸ë¦¬ê¸° ìœ„í•´ ì¶”ê°€

# KoNLPy (í˜•íƒœì†Œ ë¶„ì„ê¸°) ì„í¬íŠ¸
# Javaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤.
try:
    from konlpy.tag import Okt
except ImportError:
    st.error("KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install konlpy'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
except Exception as e:
    st.error(f"KoNLPy ì´ˆê¸°í™” ì˜¤ë¥˜ (Java ì„¤ì¹˜ í™•ì¸ í•„ìš”): {e}")

# -----------------------------
# 1. ì„¤ì • ë° ë¶ˆìš©ì–´
# -----------------------------

# ì£¼ì‹ ì»¤ë®¤ë‹ˆí‹°ìš© í™•ì¥ ë¶ˆìš©ì–´
DEFAULT_STOPWORDS: Set[str] = {
    "ê·¸ëƒ¥", "ê·¼ë°", "ê·¸ë¦¬ê³ ", "ë˜", "ì¢€", "ì´ê±°", "ì €ê±°", "ê±°ì˜",
    "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ", "ê·¸ëŸ¼", "ì œë°œ", "ì§„ì§œ", "ì¡´ë‚˜", 
    "ì‹œë°œ", "ë³‘ì‹ ", "í˜•ë“¤", "í˜•ë‹˜", "ê°œì¶”", "ë¹„ì¶”", "ì •ë„", "ë•Œë¬¸", 
    "ì‚¬ëŒ", "ìƒê°", "ë¬´ìŠ¨", "ì–´ë–»ê²Œ", "ì™œ", "ë‹¤ì‹œ", "ê³„ì†", "ë‚˜", "ë„ˆ", "ìš°ë¦¬",
    "í•˜ë‚˜", "ì§€ê¸ˆ", "ë³´ê³ ", "ê°€ì§€", "ë‹¬ëŸ¬", "ì£¼ì‹", "ì‹œì¥"
}

# -----------------------------
# 2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ (í˜•íƒœì†Œ ë¶„ì„ ì ìš©)
# -----------------------------

@st.cache_resource
def get_tokenizer():
    """
    Okt ì¸ìŠ¤í„´ìŠ¤ëŠ” ë¡œë”©ì— ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ìºì‹±í•˜ì—¬ ì‚¬ìš©
    """
    return Okt()

def tokenize_text_korean(
    text: str,
    stopwords: Optional[Set[str]] = None,
    min_len: int = 2,
) -> List[str]:
    """
    KoNLPy(Okt)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ë§Œ ì¶”ì¶œ.
    - êµì°©ì–´ íŠ¹ì„±ìƒ ë‹¨ìˆœ ë„ì–´ì“°ê¸°ê°€ ì•„ë‹Œ 'ëª…ì‚¬' ì¶”ì¶œì´ í•µì‹¬.
    """
    if not isinstance(text, str):
        return []
    
    stopwords = stopwords or set()
    
    # 1. ê¸°ë³¸ì ì¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±)
    # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì œê±°
    cleaned_text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    
    # 2. í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬ ì¶”ì¶œ)
    try:
        okt = get_tokenizer()
        nouns = okt.nouns(cleaned_text) # ëª…ì‚¬ë§Œ ì¶”ì¶œ
    except Exception:
        # Java ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ splitìœ¼ë¡œ ëŒ€ì²´ (Fall-back)
        nouns = cleaned_text.split()

    # 3. ì˜ë¬¸ ì²˜ë¦¬ (OktëŠ” ì˜ë¬¸ì„ ì˜ ëª» ì¡ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì¶”ì¶œí•´ì„œ í•©ì¹  ìˆ˜ë„ ìˆìŒ)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ Okt ê²°ê³¼ + ì›ë¬¸ì˜ ì˜ë‹¨ì–´(ì†Œë¬¸ì)ë¥¼ ë³‘í•©í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
    english_tokens = re.findall(r"[a-zA-Z]+", text)
    english_tokens = [t.lower() for t in english_tokens]
    
    # 4. ìµœì¢… í•„í„°ë§
    final_tokens = []
    
    # í•œê¸€ ëª…ì‚¬ í•„í„°ë§
    for n in nouns:
        if len(n) >= min_len and n not in stopwords:
            final_tokens.append(n)
            
    # ì˜ë¬¸ í† í° í•„í„°ë§
    for e in english_tokens:
        if len(e) >= min_len and e not in stopwords:
            final_tokens.append(e)

    return final_tokens


# -----------------------------
# 3. í¬ë¡¤ëŸ¬ (ì°¨ë‹¨ ë°©ì§€ ê¸°ëŠ¥ ì¶”ê°€)
# -----------------------------

def crawl_dc_minor_v2(
    gallery_id: str,
    start_page: int,
    end_page: int,
    min_delay: float = 0.5,
    max_delay: float = 1.5,
) -> pd.DataFrame:
    """
    ë””ì‹œ ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ (ëœë¤ ë”œë ˆì´ ì ìš©)
    """
    # User-Agentë¥¼ ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    rows = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_pages = end_page - start_page + 1
    
    for idx, page in enumerate(range(start_page, end_page + 1)):
        status_text.text(f"í˜„ì¬ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
        progress_bar.progress((idx) / total_pages)
        
        list_url = f"https://gall.dcinside.com/mgallery/board/lists/?id={gallery_id}&page={page}"
        
        try:
            res = requests.get(list_url, headers=headers, timeout=10)
            res.raise_for_status()
        except Exception as e:
            st.warning(f"í˜ì´ì§€ {page} ì ‘ì† ì‹¤íŒ¨: {e}")
            continue

        soup = BeautifulSoup(res.text, "html.parser")
        trs = soup.select("tr.ub-content.us-post") or soup.select("tr.ub-content")

        for tr in trs:
            a_tag = tr.select_one("a.ub-word")
            if a_tag is None:
                continue

            title = a_tag.get_text(strip=True)
            href = a_tag.get("href")
            if not href:
                continue
            
            # ë§í¬ ë³´ì •
            if href.startswith("//"):
                post_url = "https:" + href
            elif href.startswith("/"):
                post_url = "https://gall.dcinside.com" + href
            else:
                post_url = href

            # ë‚ ì§œ
            date_td = tr.select_one("td.gall_date")
            timestamp_text = date_td.get("title") or date_td.get_text(strip=True) if date_td else ""

            # ë³¸ë¬¸ ìˆ˜ì§‘ (ëœë¤ ë”œë ˆì´)
            content_text = ""
            try:
                # ë„ˆë¬´ ë¹ ë¥´ì§€ ì•Šê²Œ ì‰¼
                time.sleep(random.uniform(min_delay, max_delay))
                
                pres = requests.get(post_url, headers=headers, timeout=5)
                if pres.status_code == 200:
                    psoup = BeautifulSoup(pres.text, "html.parser")
                    content_div = psoup.select_one("div.write_div")
                    if content_div:
                        content_text = content_div.get_text(separator=" ", strip=True)
            except Exception:
                pass # ë³¸ë¬¸ ì‹¤íŒ¨í•´ë„ ì œëª©ì´ë¼ë„ ê±´ì§

            rows.append({
                "timestamp_str": timestamp_text,
                "title": title,
                "content": content_text,
                "url": post_url
            })
            
        # í˜ì´ì§€ ë„˜ì–´ê°ˆ ë•Œë„ ë”œë ˆì´
        time.sleep(random.uniform(min_delay, max_delay))

    progress_bar.progress(1.0)
    status_text.text("ìˆ˜ì§‘ ì™„ë£Œ!")
    
    if not rows:
        return pd.DataFrame()

    df = pd.DataFrame(rows)
    
    # ë‚ ì§œ íŒŒì‹± ë¡œì§
    def parse_ts(x):
        x = str(x).strip()
        patterns = ["%Y.%m.%d %H:%M:%S", "%Y.%m.%d %H:%M", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"]
        for pat in patterns:
            try:
                return pd.to_datetime(x, format=pat)
            except:
                continue
        # ì˜¤ëŠ˜ ë‚ ì§œ(HH:mm)ì¸ ê²½ìš° ì²˜ë¦¬ ë“±ì€ ìƒëµí•˜ê³  NaT ì²˜ë¦¬
        return pd.NaT

    df["timestamp"] = df["timestamp_str"].apply(parse_ts)
    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ í–‰(ì˜¤ë˜ëœ ê¸€ì´ë‚˜ í˜•ì‹ ë‹¤ë¥¸ ê¸€) ì œê±° í˜¹ì€ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°€ì •
    df = df.dropna(subset=["timestamp"])
    df["date"] = df["timestamp"].dt.date
    
    return df


# -----------------------------
# 4. í†µê³„ ìƒì„± (ì¼ìë³„)
# -----------------------------

def build_stats_v2(df_posts: pd.DataFrame):
    """
    ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ (date, word) ë¹ˆë„ í…Œì´ë¸” ìƒì„±
    """
    all_rows = []
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    prog = st.progress(0)
    total_len = len(df_posts)
    
    for i, row in df_posts.iterrows():
        if i % 10 == 0:
            prog.progress(min(i / total_len, 1.0))
            
        full_text = str(row["title"]) + " " + str(row["content"])
        tokens = tokenize_text_korean(full_text, stopwords=DEFAULT_STOPWORDS)
        
        for token in tokens:
            all_rows.append({
                "date": row["date"],
                "word": token
            })
            
    prog.progress(1.0)
            
    if not all_rows:
        return pd.DataFrame()
        
    df_tokens = pd.DataFrame(all_rows)
    
    # ë‚ ì§œë³„, ë‹¨ì–´ë³„ ì¹´ìš´íŠ¸
    df_daily = df_tokens.groupby(["date", "word"]).size().reset_index(name="count")
    
    # í•´ë‹¹ ë‚ ì§œì˜ ì´ ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ë¹ˆë„ìœ¨ freq ê³„ì‚°ìš©)
    daily_total = df_tokens.groupby("date").size().reset_index(name="total_words")
    df_daily = df_daily.merge(daily_total, on="date", how="left")
    df_daily["freq"] = df_daily["count"] / df_daily["total_words"]
    
    return df_daily


# -----------------------------
# 5. ë©”ì¸ UI
# -----------------------------

def main():
    st.set_page_config(page_title="ì£¼ì‹ ì‹¬ë¦¬ ë¶„ì„ê¸° V2", layout="wide")
    
    st.title("ğŸ§  ì£¼ì‹ ì»¤ë®¤ë‹ˆí‹° ì‹¬ë¦¬ ë¶„ì„ê¸° V2")
    st.caption("ë””ì‹œì¸ì‚¬ì´ë“œ ë¯¸ì£¼ê°¤ ë°ì´í„° ê¸°ë°˜ Â· KoNLPy í˜•íƒœì†Œ ë¶„ì„ Â· ì‹œê³„ì—´ íŠ¸ë Œë“œ ì¶”ì ")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "df_posts" not in st.session_state:
        st.session_state["df_posts"] = pd.DataFrame()
    if "df_daily" not in st.session_state:
        st.session_state["df_daily"] = pd.DataFrame()

    # --- ì‚¬ì´ë“œë°”: ë°ì´í„° ìˆ˜ì§‘ ---
    with st.sidebar:
        st.header("1. ë°ì´í„° ìˆ˜ì§‘")
        
        # ê°¤ëŸ¬ë¦¬ ID (ê¸°ë³¸ê°’: ë¯¸ì£¼ê°¤)
        gallery_id = st.text_input("ê°¤ëŸ¬ë¦¬ ID", value="stockus") 
        col1, col2 = st.columns(2)
        start_p = col1.number_input("ì‹œì‘ í˜ì´ì§€", 1, 1000, 1)
        end_p = col2.number_input("ì¢…ë£Œ í˜ì´ì§€", 1, 1000, 3) # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ê²Œ ì„¤ì •
        
        if st.button("ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘"):
            with st.spinner("ë””ì”¨ ë°©ë¬¸ ì¤‘... (ëœë¤ ë”œë ˆì´ ì ìš©ë¨)"):
                df_new = crawl_dc_minor_v2(gallery_id, start_p, end_p)
                
            if not df_new.empty:
                st.session_state["df_posts"] = df_new
                st.success(f"{len(df_new)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ìˆ˜ì§‘ í›„ ë°”ë¡œ ë¶„ì„ ì‹¤í–‰
                with st.spinner("í˜•íƒœì†Œ ë¶„ì„(KoNLPy) ìˆ˜í–‰ ì¤‘..."):
                    df_stats = build_stats_v2(df_new)
                    st.session_state["df_daily"] = df_stats
                st.success("ë¶„ì„ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # --- ë©”ì¸ í™”ë©´ ---
    
    df_daily = st.session_state["df_daily"]

    if df_daily.empty:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ í¬ë¡¤ë§ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ì£¼ìš” í‚¤ì›Œë“œ(Bar)", "ğŸ“ˆ ì‹¬ë¦¬/í…Œë§ˆ íŠ¸ë Œë“œ(Line)", "ğŸ“ ì›ë³¸ ë°ì´í„°"])

    # 1. ì£¼ìš” í‚¤ì›Œë“œ ë­í‚¹
    with tab1:
        st.subheader("ê¸°ê°„ ë‚´ ìµœë‹¤ ì–¸ê¸‰ ë‹¨ì–´")
        
        top_n = st.slider("ìƒìœ„ Nê°œ ë³´ê¸°", 10, 50, 20)
        
        # ì „ì²´ ê¸°ê°„ í•©ì‚°
        total_counts = df_daily.groupby("word")["count"].sum().reset_index()
        total_counts = total_counts.sort_values("count", ascending=False).head(top_n)
        
        fig = px.bar(total_counts, x="word", y="count", 
                     title=f"Top {top_n} í‚¤ì›Œë“œ", color="count")
        st.plotly_chart(fig, use_container_width=True)

    # 2. íŠ¸ë Œë“œ ë¶„ì„ (í•µì‹¬ ê¸°ëŠ¥)
    with tab2:
        st.subheader("ê´€ì‹¬ í‚¤ì›Œë“œ ì‹œê³„ì—´ ì¶”ì ")
        st.caption("íŠ¹ì • ì£¼ì‹ì´ë‚˜ ê°ì • ë‹¨ì–´ê°€ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        # ê²€ìƒ‰ ê¸°ëŠ¥
        all_words = sorted(df_daily["word"].unique())
        default_keywords = ["í…ŒìŠ¬ë¼", "ì—”ë¹„ë””ì•„", "ë¡±", "ìˆ", "ì¡¸ì—…", "í•œê°•"]
        # ë°ì´í„°ì— ìˆëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§
        valid_defaults = [w for w in default_keywords if w in all_words]
        
        selected_words = st.multiselect("ì¶”ì í•  ë‹¨ì–´ë¥¼ ì„ íƒ/ì…ë ¥í•˜ì„¸ìš”", all_words, default=valid_defaults)
        
        if selected_words:
            # ì„ íƒëœ ë‹¨ì–´ë§Œ í•„í„°ë§
            mask = df_daily["word"].isin(selected_words)
            chart_df = df_daily[mask].copy()
            
            # ë‚ ì§œ ì •ë ¬
            chart_df = chart_df.sort_values("date")
            
            # ë¼ì¸ ì°¨íŠ¸ ê·¸ë¦¬ê¸°
            # yì¶•ì„ 'freq'(ë¹„ìœ¨)ë¡œ í•˜ë©´ ê²Œì‹œê¸€ ìˆ˜ê°€ ë‹¤ë¥¸ ë‚ ì§œë¼ë¦¬ ë¹„êµí•˜ê¸° ë” ì¢‹ìŒ
            metric = st.radio("ì§€í‘œ ì„ íƒ", ["count (ë‹¨ìˆœ íšŸìˆ˜)", "freq (ì–¸ê¸‰ ë°€ë„)"], index=0)
            y_col = "count" if "count" in metric else "freq"
            
            fig2 = px.line(chart_df, x="date", y=y_col, color="word", markers=True,
                           title="í‚¤ì›Œë“œë³„ ì–¸ê¸‰ ì¶”ì´ ë³€í™”")
            st.plotly_chart(fig2, use_container_width=True)
            
            st.markdown("""
            **ğŸ’¡ ë¶„ì„ íŒ:**
            - **ê¸‰ë“±:** í‰ì†Œ ì ì í•˜ë˜ ì¢…ëª©ì´ ê°‘ìê¸° ì–¸ê¸‰ëŸ‰ì´ í­ë°œí•˜ë©´ 'ì¬ë£Œ'ê°€ ë–´ê±°ë‚˜ 'ê³¼ì—´' ì§•ì¡°ì…ë‹ˆë‹¤.
            - **ê°ì •:** 'ì¡¸ì—…'(ìˆ˜ìµì‹¤í˜„), 'í•œê°•'(ì†ì‹¤) ê°™ì€ ë‹¨ì–´ì™€ ì¢…ëª©ëª…ì˜ ì¶”ì´ë¥¼ ê²¹ì³ë³´ì„¸ìš”.
            """)
        else:
            st.info("ì¶”ì í•  ë‹¨ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

    # 3. ë°ì´í„° í™•ì¸
    with tab3:
        st.dataframe(st.session_state["df_posts"])

if __name__ == "__main__":
    main()