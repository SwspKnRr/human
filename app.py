import time
import re
from typing import List, Set, Optional, Literal

import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st


# -----------------------------
# ì „ì—­ ì„¤ì •
# -----------------------------

# ì•„ì£¼ ê¸°ë³¸ì ì¸ ë¶ˆìš©ì–´ (í•„ìš”í•  ë•Œ ì¶”ê°€í•´ê°€ë©´ ë¨)
DEFAULT_STOPWORDS: Set[str] = {
    "ê·¸ëƒ¥", "ê·¼ë°", "ê·¸ë¦¬ê³ ", "ë˜", "ì¢€", "ì´ê±°", "ì €ê±°", "ê±°ì˜",
    "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ", "ê·¸ëŸ¼", "ì œë°œ",
    "the", "and", "or", "but", "a", "an", "to", "of",
}

# ìˆ«ì/ì˜ë¬¸/í•œê¸€/ì´ˆì„±ê¹Œì§€ í—ˆìš©
TOKEN_PATTERN = re.compile(r"[0-9A-Za-zê°€-í£ã„±-ã…ã…-ã…£]+")


# -----------------------------
# 1. í…ìŠ¤íŠ¸ í† í°í™”
# -----------------------------

def tokenize_text(
    text: str,
    stopwords: Optional[Set[str]] = None,
    min_len: int = 2,
) -> List[str]:
    """
    ê²Œì‹œê¸€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    - ì˜ë¬¸ì€ ì†Œë¬¸ìë¡œ
    - min_len ì´í•˜ í† í° ì œê±°
    - stopwords ì œê±°
    """
    if not isinstance(text, str):
        return []
    stopwords = stopwords or set()
    tokens: List[str] = []

    for match in TOKEN_PATTERN.finditer(text):
        token = match.group(0)

        # ì˜ë¬¸ì€ ì†Œë¬¸ìë¡œ
        if re.fullmatch(r"[A-Za-z]+", token):
            token = token.lower()

        if len(token) < min_len:
            continue
        if token in stopwords:
            continue

        tokens.append(token)

    return tokens


# -----------------------------
# 2. ë””ì”¨ ë¯¸ì£¼ê°¤ í¬ë¡¤ëŸ¬ (ê°„ë‹¨ ë²„ì „)
# -----------------------------

def crawl_dc_minor(
    gallery_id: str,
    start_page: int,
    end_page: int,
    delay: float = 1.0,
) -> pd.DataFrame:
    """
    ë””ì‹œ ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬(list â†’ ê¸€ ë³¸ë¬¸)ë¥¼ ê°„ë‹¨ í¬ë¡¤ë§.

    - gallery_id ì˜ˆ: 'us_stock' (ì‹¤ì œ ê°¤ ì£¼ì†Œ í™•ì¸ í•„ìš”)
    - start_page, end_page: ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë²”ìœ„ (1ë¶€í„° ì‹œì‘)
    - ë„ˆë¬´ í° ë²”ìœ„ ë„£ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¬ê³ , ì‚¬ì´íŠ¸ì— ë¶€ë‹´ ì¤„ ìˆ˜ ìˆìœ¼ë‹ˆ ì ë‹¹íˆ.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; stock-sentiment-bot/0.1; +https://example.com)"
    }

    rows = []

    for page in range(start_page, end_page + 1):
        list_url = f"https://gall.dcinside.com/mgallery/board/lists/?id={gallery_id}&page={page}"
        try:
            res = requests.get(list_url, headers=headers, timeout=10)
            res.raise_for_status()
        except Exception as e:
            st.warning(f"ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        soup = BeautifulSoup(res.text, "html.parser")

        # ê²Œì‹œê¸€ í–‰ ì„ íƒ (í´ë˜ìŠ¤ëª…ì€ ì‹¤ì œ HTML ë³´ê³  í•„ìš”í•˜ë©´ ì¡°ì •)
        trs = soup.select("tr.ub-content.us-post") or soup.select("tr.ub-content")

        for tr in trs:
            # ì œëª©, ë§í¬
            a_tag = tr.select_one("a.ub-word")
            if a_tag is None:
                continue

            title = a_tag.get_text(strip=True)
            href = a_tag.get("href")
            if not href:
                continue

            # ë§í¬ ë³´ì •
            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = "https://gall.dcinside.com" + href
            post_url = href

            # ì‘ì„± ì‹œê° (ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš°)
            date_td = tr.select_one("td.gall_date")
            if date_td is None:
                timestamp_text = ""
            else:
                # ë³´í†µ title ì†ì„±ì— ì „ì²´ ì‹œê°, í…ìŠ¤íŠ¸ì—ëŠ” ì‹œ/ë‚ ì§œ ì¼ë¶€ë§Œ ìˆìŒ
                timestamp_text = date_td.get("title") or date_td.get_text(strip=True)

            # ê¸€ ë³¸ë¬¸ ìš”ì²­
            content_text = ""
            try:
                time.sleep(delay)
                pres = requests.get(post_url, headers=headers, timeout=10)
                pres.raise_for_status()
                psoup = BeautifulSoup(pres.text, "html.parser")
                # ë³¸ë¬¸ ì˜ì—­ (ì—­ì‹œ ì‹¤ì œ HTML ë³´ê³  í´ë˜ìŠ¤ëª… ì¡°ì • ê°€ëŠ¥)
                content_div = psoup.select_one("div.write_div")
                if content_div:
                    content_text = content_div.get_text(separator=" ", strip=True)
            except Exception as e:
                st.warning(f"ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {post_url}, ì˜¤ë¥˜: {e}")

            rows.append(
                {
                    "timestamp_raw": timestamp_text,
                    "title": title,
                    "content": content_text,
                    "url": post_url,
                    "page": page,
                }
            )

        time.sleep(delay)

    if not rows:
        return pd.DataFrame(columns=["timestamp", "title", "content", "url", "page"])

    df = pd.DataFrame(rows)

    # timestamp íŒŒì‹± (í˜•ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ëª‡ ê°€ì§€ íŒ¨í„´ ì‹œë„)
    def parse_ts(x: str):
        import datetime as dt
        x = (x or "").strip()
        for fmt in ("%Y.%m.%d %H:%M", "%Y-%m-%d %H:%M", "%Y.%m.%d", "%Y-%m-%d"):
            try:
                return dt.datetime.strptime(x, fmt)
            except Exception:
                continue
        return pd.NaT

    df["timestamp"] = df["timestamp_raw"].apply(parse_ts)
    df = df.dropna(subset=["timestamp"]).reset_index(drop=True)

    return df[["timestamp", "title", "content", "url", "page"]]


# -----------------------------
# 3. ì¼ìë³„ ë‹¨ì–´ í†µê³„ ë§Œë“¤ê¸°
# -----------------------------

def build_daily_word_stats(
    df_posts: pd.DataFrame,
    stopwords: Optional[Set[str]] = None,
    min_len: int = 2,
) -> pd.DataFrame:
    """
    raw posts DataFrame â†’ (date, word) ë‹¨ìœ„ ì¼ìë³„ í†µê³„ë¡œ ë³€í™˜
    """
    if df_posts.empty:
        return pd.DataFrame(
            columns=["date", "word", "count", "freq", "total_words", "total_posts"]
        )

    df = df_posts.copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["date"] = df["timestamp"].dt.date

    # title + content í•©ì¹˜ê¸°
    df["text"] = (
        df.get("title", "").fillna("").astype(str)
        + " "
        + df.get("content", "").fillna("").astype(str)
    )

    df["tokens"] = df["text"].apply(
        lambda x: tokenize_text(x, stopwords=stopwords or DEFAULT_STOPWORDS, min_len=min_len)
    )
    df["token_count"] = df["tokens"].apply(len)

    daily_stats = []

    for date, grp in df.groupby("date"):
        total_posts = len(grp)
        total_words = int(grp["token_count"].sum())

        exploded = grp[["tokens"]].explode("tokens")
        exploded = exploded.dropna(subset=["tokens"])

        if exploded.empty:
            continue

        word_group = exploded.groupby("tokens", as_index=False).size()
        word_group = word_group.rename(columns={"tokens": "word", "size": "count"})

        if total_words > 0:
            word_group["freq"] = word_group["count"] / total_words
        else:
            word_group["freq"] = 0.0

        word_group["date"] = date
        word_group["total_words"] = total_words
        word_group["total_posts"] = total_posts

        daily_stats.append(word_group)

    if not daily_stats:
        return pd.DataFrame(
            columns=["date", "word", "count", "freq", "total_words", "total_posts"]
        )

    df_daily = pd.concat(daily_stats, ignore_index=True)
    df_daily = df_daily[["date", "word", "count", "freq", "total_words", "total_posts"]]
    return df_daily


# -----------------------------
# 4. ì¡°íšŒ í•¨ìˆ˜ë“¤
# -----------------------------

def get_range_word_stats(
    df_daily: pd.DataFrame,
    start_date: str,
    end_date: str,
    min_days: int = 1,
    top_n: int = 50,
    sort_by: Literal["sum_count", "avg_freq", "max_freq"] = "sum_count",
) -> pd.DataFrame:
    """
    íŠ¹ì • ê¸°ê°„ [start_date, end_date] ë‚´ì—ì„œ ë‹¨ì–´ë³„ ì§‘ê³„
    """
    df = df_daily.copy()
    if not pd.api.types.is_string_dtype(df["date"]):
        df["date"] = df["date"].astype(str)

    mask = (df["date"] >= start_date) & (df["date"] <= end_date)
    sub = df.loc[mask]

    if sub.empty:
        return pd.DataFrame()

    grouped = (
        sub.groupby("word")
        .agg(
            sum_count=("count", "sum"),
            days_appeared=("date", "nunique"),
            avg_freq=("freq", "mean"),
            max_freq=("freq", "max"),
        )
        .reset_index()
    )

    grouped = grouped[grouped["days_appeared"] >= min_days]

    if grouped.empty:
        return grouped

    if sort_by == "sum_count":
        grouped = grouped.sort_values("sum_count", ascending=False)
    elif sort_by == "avg_freq":
        grouped = grouped.sort_values("avg_freq", ascending=False)
    elif sort_by == "max_freq":
        grouped = grouped.sort_values("max_freq", ascending=False)
    else:
        raise ValueError(f"invalid sort_by: {sort_by}")

    if top_n and top_n > 0:
        grouped = grouped.head(top_n)

    return grouped


def get_day_word_stats(
    df_daily: pd.DataFrame,
    target_date: str,
    min_count: int = 1,
    top_n: int = 100,
    sort_by: Literal["count", "freq"] = "count",
) -> pd.DataFrame:
    """
    íŠ¹ì • ë‚ ì§œì˜ ë‹¨ì–´ ë¶„í¬ ì¡°íšŒ
    """
    df = df_daily.copy()
    if not pd.api.types.is_string_dtype(df["date"]):
        df["date"] = df["date"].astype(str)

    sub = df[df["date"] == target_date]

    if sub.empty:
        return pd.DataFrame()

    sub = sub[sub["count"] >= min_count]

    if sub.empty:
        return sub

    if sort_by == "count":
        sub = sub.sort_values("count", ascending=False)
    elif sort_by == "freq":
        sub = sub.sort_values("freq", ascending=False)
    else:
        raise ValueError(f"invalid sort_by: {sort_by}")

    if top_n and top_n > 0:
        sub = sub.head(top_n)

    return sub.reset_index(drop=True)


# -----------------------------
# 5. Streamlit UI
# -----------------------------

def main():
    st.set_page_config(page_title="ë””ì”¨ ë¯¸ì£¼ê°¤ ë‹¨ì–´ ê´€ì°°ì‹¤", layout="wide")
    st.title("ğŸ“Š ë””ì”¨ ë¯¸êµ­ ì£¼ì‹ ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ Â· ë‹¨ì–´ ê´€ì°° ì‹¤í—˜ì‹¤ (V1)")

    # ----------------- ì‚¬ì´ë“œë°”: ë°ì´í„° ì¤€ë¹„ -----------------
    st.sidebar.header("1. ë°ì´í„° ì¤€ë¹„")

    st.sidebar.markdown("**ì˜µì…˜ A. CSV ì—…ë¡œë“œ (raw_posts)**")
    uploaded = st.sidebar.file_uploader("raw_posts CSV ì—…ë¡œë“œ", type=["csv"])

    st.sidebar.markdown("---")
    st.sidebar.markdown("**ì˜µì…˜ B. ì•±ì—ì„œ ì§ì ‘ í¬ë¡¤ë§ (ì‹¤í—˜ìš©)**")
    gallery_id = st.sidebar.text_input("ê°¤ëŸ¬ë¦¬ ID", value="stockus")
    start_page = st.sidebar.number_input("ì‹œì‘ í˜ì´ì§€", min_value=1, value=1, step=1)
    end_page = st.sidebar.number_input("ë í˜ì´ì§€", min_value=1, value=2, step=1)
    delay = st.sidebar.number_input("ìš”ì²­ ê°„ê²©(ì´ˆ)", min_value=0.0, value=1.0, step=0.5)

    crawl_button = st.sidebar.button("ë””ì”¨ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰")

    df_posts: Optional[pd.DataFrame] = None

    # CSV ì—…ë¡œë“œ ìš°ì„ 
    if uploaded is not None:
        df_posts = pd.read_csv(uploaded)
        st.success(f"CSV ì—…ë¡œë“œ ì™„ë£Œ: {len(df_posts)} rows")

    # í¬ë¡¤ë§ ì‹¤í–‰ ì‹œ
    if crawl_button:
        with st.spinner("ë””ì”¨ ë¯¸ì£¼ê°¤ì—ì„œ ê¸€ ìˆ˜ì§‘ ì¤‘... (í˜ì´ì§€ ìˆ˜ê°€ ë§ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¼)"):
            df_crawled = crawl_dc_minor(
                gallery_id=gallery_id,
                start_page=int(start_page),
                end_page=int(end_page),
                delay=float(delay),
            )
        if df_crawled.empty:
            st.error("í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê°¤ëŸ¬ë¦¬ ID / í˜ì´ì§€ ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.success(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(df_crawled)} posts")
            st.dataframe(df_crawled.head())
            if df_posts is None:
                df_posts = df_crawled
            else:
                # ì—…ë¡œë“œ + í¬ë¡¤ë§ ê°™ì´ ì“°ê³  ì‹¶ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ í•©ì¹˜ê¸°
                df_posts = pd.concat([df_posts, df_crawled], ignore_index=True)

    if df_posts is None or df_posts.empty:
        st.info("ì¢Œì¸¡ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, í¬ë¡¤ë§ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ----------------- ì¼ìë³„ ë‹¨ì–´ í†µê³„ -----------------
    st.markdown("### 2. ì¼ìë³„ ë‹¨ì–´ í†µê³„ ìƒì„±")

    if st.checkbox("ì¼ìë³„ ë‹¨ì–´ í†µê³„ ìƒˆë¡œ ê³„ì‚°í•˜ê¸°", value=True):
        with st.spinner("ì¼ìë³„ ë‹¨ì–´ í†µê³„ ê³„ì‚° ì¤‘..."):
            df_daily = build_daily_word_stats(df_posts)
        if df_daily.empty:
            st.error("ì¼ìë³„ ë‹¨ì–´ í†µê³„ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
            return
        st.success(f"ì™„ë£Œ: {df_daily['date'].nunique()}ì¼, {len(df_daily)} (date, word) rows")
        st.session_state["df_daily"] = df_daily
    else:
        df_daily = st.session_state.get("df_daily")
        if df_daily is None or df_daily.empty:
            st.warning("ì €ì¥ëœ df_dailyê°€ ì—†ìŠµë‹ˆë‹¤. í†µê³„ë¥¼ í•œ ë²ˆ ê³„ì‚°í•´ ì£¼ì„¸ìš”.")
            return

    # ----------------- íƒ­: ê¸°ê°„ / ì¼ì ëª¨ë“œ -----------------
    tab_range, tab_day = st.tabs(["ğŸ“… ê¸°ê°„ ë‹¨ì–´ ë¹ˆë„", "ğŸ“† íŠ¹ì • ë‚ ì§œ ë‹¨ì–´ ë¶„í¬"])

    # ----- íƒ­ 1: ê¸°ê°„ ë‹¨ì–´ ë¹ˆë„ -----
    with tab_range:
        st.subheader("ê¸°ê°„ ë‹¨ì–´ ë¹ˆë„")

        col1, col2 = st.columns(2)
        min_date = pd.to_datetime(df_daily["date"]).min()
        max_date = pd.to_datetime(df_daily["date"]).max()
        with col1:
            start = st.date_input("ì‹œì‘ ë‚ ì§œ", value=min_date, min_value=min_date, max_value=max_date)
        with col2:
            end = st.date_input("ë ë‚ ì§œ", value=max_date, min_value=min_date, max_value=max_date)

        col3, col4, col5 = st.columns(3)
        with col3:
            min_days = st.number_input("ìµœì†Œ ë“±ì¥ ì¼ìˆ˜", min_value=1, value=1)
        with col4:
            top_n = st.number_input("í‘œì‹œ ë‹¨ì–´ ìˆ˜ (Top N)", min_value=10, max_value=300, value=50, step=10)
        with col5:
            sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["sum_count", "avg_freq", "max_freq"])

        if st.button("ê¸°ê°„ ë‹¨ì–´ ë¹ˆë„ ì¡°íšŒ"):
            start_str = start.strftime("%Y-%m-%d")
            end_str = end.strftime("%Y-%m-%d")
            stats = get_range_word_stats(
                df_daily,
                start_date=start_str,
                end_date=end_str,
                min_days=int(min_days),
                top_n=int(top_n),
                sort_by=sort_by,  # type: ignore[arg-type]
            )
            if stats.empty:
                st.warning("ì¡°ê±´ì— ë§ëŠ” ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.write(f"ì„ íƒ ê¸°ê°„: {start_str} ~ {end_str}")
                st.dataframe(stats)

                st.markdown("#### ìƒìœ„ ë‹¨ì–´ ë§‰ëŒ€ ê·¸ë˜í”„ (sum_count ê¸°ì¤€)")
                chart_data = stats.set_index("word")["sum_count"]
                st.bar_chart(chart_data)

    # ----- íƒ­ 2: íŠ¹ì • ë‚ ì§œ ë‹¨ì–´ ë¶„í¬ -----
    with tab_day:
        st.subheader("íŠ¹ì • ë‚ ì§œ ë‹¨ì–´ ë¶„í¬")

        all_dates = sorted(pd.to_datetime(df_daily["date"]).unique())
        default_date = all_dates[-1] if all_dates else None
        target = st.date_input("ë‚ ì§œ ì„ íƒ", value=default_date)

        col1, col2, col3 = st.columns(3)
        with col1:
            min_count = st.number_input("ìµœì†Œ ë“±ì¥ íšŸìˆ˜", min_value=1, value=3)
        with col2:
            top_n_day = st.number_input("í‘œì‹œ ë‹¨ì–´ ìˆ˜ (Top N)", min_value=10, max_value=300, value=50, step=10)
        with col3:
            sort_by_day = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["count", "freq"])

        if st.button("í•´ë‹¹ ë‚ ì§œ ë‹¨ì–´ ë¶„í¬ ì¡°íšŒ"):
            t_str = target.strftime("%Y-%m-%d")
            day_stats = get_day_word_stats(
                df_daily,
                target_date=t_str,
                min_count=int(min_count),
                top_n=int(top_n_day),
                sort_by=sort_by_day,  # type: ignore[arg-type]
            )
            if day_stats.empty:
                st.warning("ì¡°ê±´ì— ë§ëŠ” ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.write(f"ì„ íƒ ë‚ ì§œ: {t_str}")
                st.dataframe(day_stats)

                st.markdown("#### ë‹¨ì–´ ë§‰ëŒ€ ê·¸ë˜í”„")
                chart_data = day_stats.set_index("word")["count"]
                st.bar_chart(chart_data)


if __name__ == "__main__":
    main()
