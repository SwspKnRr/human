import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
import random
import datetime
import yfinance as yf
import mplfinance as mpf
import matplotlib.pyplot as plt

# ---------------------------------------------------------
# ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
# ---------------------------------------------------------
st.set_page_config(
    page_title="ë””ì”¨ ì‹¬ë¦¬ vs ì£¼ê°€ ë¶„ì„ê¸°",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (Matplotlib)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

def simple_tokenizer(text):
    """ê°„ë‹¨í•œ ë„ì–´ì“°ê¸° ê¸°ë°˜ í† í¬ë‚˜ì´ì € (KoNLPy ì˜ì¡´ì„± ì œê±°)"""
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    return text.split()

# ---------------------------------------------------------
# í¬ë¡¤ë§ í•¨ìˆ˜ (V10 ë¡œì§ ì´ì‹)
# ---------------------------------------------------------
def crawl_dc(gallery_id, gallery_type, start_page, end_page, is_fast_mode):
    base_url = "https://gall.dcinside.com"
    if gallery_type == "minor":
        list_url = f"{base_url}/mgallery/board/lists/"
        view_url = f"{base_url}/mgallery/board/view/"
    elif gallery_type == "mini":
        list_url = f"{base_url}/mini/board/lists/"
        view_url = f"{base_url}/mini/board/view/"
    else: # major
        list_url = f"{base_url}/board/lists/"
        view_url = f"{base_url}/board/view/"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": base_url,
        "Connection": "keep-alive"
    }
    
    session = requests.Session()
    session.headers.update(headers)
    
    rows = []
    
    # Streamlit ìƒíƒœ í‘œì‹œì¤„
    status_text = st.empty()
    progress_bar = st.progress(0)
    total_pages = end_page - start_page + 1

    for idx, page in enumerate(range(start_page, end_page + 1)):
        status_text.text(f"ğŸ” {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... ({gallery_id})")
        
        try:
            res = session.get(list_url, params={'id': gallery_id, 'page': page}, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            
            trs = soup.select("tbody tr")
            if not trs: trs = soup.select("tr")
            
            count_in_page = 0
            for tr in trs:
                # ì œëª© íƒœê·¸ ì°¾ê¸°
                a_tag = tr.select_one("a.ub-word")
                if not a_tag:
                    links = tr.select("a")
                    for l in links:
                        href = l.get("href", "")
                        if "board/view" in href and "no=" in href:
                            a_tag = l
                            break
                if not a_tag: continue

                title = a_tag.get_text(strip=True)
                link = a_tag.get("href")
                
                if "ê³µì§€" in title or "ì„¤ë¬¸" in title: continue
                
                # ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
                dt = tr.select_one("td.gall_date")
                date_str = ""
                if dt: date_str = dt.get("title") or dt.get_text(strip=True)

                # ë³¸ë¬¸ ìˆ˜ì§‘ (ìŠ¤í”¼ë“œ ëª¨ë“œ ì•„ë‹ ë•Œë§Œ)
                content = ""
                if not is_fast_mode:
                    match = re.search(r'no=([0-9]+)', link)
                    if match:
                        post_no = match.group(1)
                        post_link = f"{view_url}?id={gallery_id}&no={post_no}"
                        try:
                            time.sleep(random.uniform(0.1, 0.3))
                            pr = session.get(post_link, timeout=5)
                            ps = BeautifulSoup(pr.text, "html.parser")
                            cd = ps.select_one("div.write_div")
                            if cd: content = cd.get_text(separator=" ", strip=True)
                        except: pass
                
                rows.append({
                    "raw_date": date_str,
                    "title": title,
                    "content": content
                })
                count_in_page += 1
                
            time.sleep(random.uniform(0.5, 0.8)) # ì°¨ë‹¨ ë°©ì§€ ë”œë ˆì´
            progress_bar.progress((idx + 1) / total_pages)
            
        except Exception as e:
            st.error(f"Error on page {page}: {e}")
            
    status_text.text(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(rows)}ê°œ ê²Œì‹œê¸€.")
    progress_bar.empty()
    
    return pd.DataFrame(rows)

def analyze_data(df):
    """ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ë‚ ì§œë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„"""
    def parse_date(x):
        x = str(x).strip()
        if re.match(r"\d{2}:\d{2}", x): return datetime.datetime.now().date()
        for fmt in ["%Y.%m.%d", "%Y-%m-%d"]:
            try: return datetime.datetime.strptime(x, fmt).date()
            except: continue
        return datetime.datetime.now().date()

    if 'raw_date' in df.columns:
        df['date'] = df['raw_date'].apply(parse_date)
    else:
        df['date'] = datetime.datetime.now().date()

    stopwords = {"ê·¸ëƒ¥", "ê·¼ë°", "ì§„ì§œ", "ì¡´ë‚˜", "ì‹œë°œ", "ìƒê°", "ì‚¬ëŒ", "ì˜¤ëŠ˜", "ì§€ê¸ˆ", "ì£¼ì‹", "ë§¤ìˆ˜", "ë§¤ë„", "ì •ë„", "ë•Œë¬¸", "ì´ê±°", "ì €ê±°", "ì–´ë–»ê²Œ", "ì™œ", "ë‹¤ì‹œ", "í•˜ë‚˜", "ë­ëƒ", "ì•„ë‹ˆ", "ë‚´ê°€", "í˜•ë“¤"}
    
    all_data = []
    for i, row in df.iterrows():
        text = f"{row['title']} {row['content']}"
        tokens = simple_tokenizer(text)
        tokens = [t for t in tokens if len(t) >= 2 and t not in stopwords]
        for t in tokens:
            all_data.append({"date": row['date'], "word": t})
            
    if not all_data: return pd.DataFrame()
    return pd.DataFrame(all_data).groupby(['date', 'word']).size().reset_index(name='count')

# ---------------------------------------------------------
# ë©”ì¸ UI
# ---------------------------------------------------------

st.title("ğŸ“ˆ ë””ì”¨ ì‹¬ë¦¬ vs ì£¼ê°€ ìº”ë“¤ ë¶„ì„ê¸°")
st.markdown("í¬ë¡¤ë§ ë°ì´í„°ì™€ Yahoo Finance ì£¼ê°€ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ **ì¸ê°„ ì§€í‘œ**ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("1. ìˆ˜ì§‘ ì„¤ì •")
    gallery_id = st.text_input("ê°¤ëŸ¬ë¦¬ ID", "stockus")
    gallery_type = st.selectbox("ê°¤ëŸ¬ë¦¬ ì¢…ë¥˜", ["minor", "major", "mini"], index=0)
    
    col1, col2 = st.columns(2)
    start_page = col1.number_input("ì‹œì‘ í˜ì´ì§€", 1, 1000, 1)
    end_page = col2.number_input("ë í˜ì´ì§€", 1, 1000, 10)
    
    is_fast_mode = st.checkbox("âš¡ ìŠ¤í”¼ë“œ ëª¨ë“œ (ì œëª©ë§Œ)", value=True, help="ì²´í¬í•˜ë©´ ì†ë„ê°€ 50ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤.")
    
    if st.button("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", type="primary"):
        with st.spinner("ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
            df_posts = crawl_dc(gallery_id, gallery_type, start_page, end_page, is_fast_mode)
            if not df_posts.empty:
                df_daily = analyze_data(df_posts)
                st.session_state['df_posts'] = df_posts
                st.session_state['df_daily'] = df_daily
                st.success("ë¶„ì„ ì™„ë£Œ!")
            else:
                st.error("ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.header("2. íŒŒì¼ ê´€ë¦¬")
    
    # ì €ì¥ ê¸°ëŠ¥
    if 'df_daily' in st.session_state and not st.session_state['df_daily'].empty:
        csv = st.session_state['df_daily'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ’¾ ë¶„ì„ ë°ì´í„°(CSV) ë‹¤ìš´ë¡œë“œ", csv, "sentiment_data.csv", "text/csv")
    
    # ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥
    uploaded_file = st.file_uploader("ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (CSV)", type="csv")
    if uploaded_file is not None:
        try:
            df_loaded = pd.read_csv(uploaded_file)
            df_loaded['date'] = pd.to_datetime(df_loaded['date']).dt.date
            st.session_state['df_daily'] = df_loaded
            st.success(f"ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ! ({len(df_loaded)} rows)")
        except Exception as e:
            st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")

# ë©”ì¸ í™”ë©´
if 'df_daily' in st.session_state and not st.session_state['df_daily'].empty:
    df_daily = st.session_state['df_daily']
    
    tab1, tab2 = st.tabs(["ğŸ•¯ï¸ ìº”ë“¤ ì°¨íŠ¸ ë¶„ì„", "ğŸ“ ì›ë³¸ ë°ì´í„°"])
    
    with tab1:
        st.subheader("ì‹¬ë¦¬ vs ì£¼ê°€ ìƒê´€ê´€ê³„ ë¶„ì„")
        
        c1, c2, c3 = st.columns([1, 1, 1])
        with c1:
            ticker = st.text_input("Yahoo Ticker", "TSLA", help="ì˜ˆ: TSLA, NVDA, AAPL, BTC-USD")
        with c2:
            # ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ ìë™ ì¶”ì²œ
            top_word = df_daily.groupby('word')['count'].sum().idxmax()
            keyword = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œ", top_word)
        with c3:
            st.write("") # ì—¬ë°±ìš©
            st.write("") 
            draw_btn = st.button("ì°¨íŠ¸ ê·¸ë¦¬ê¸°")

        if draw_btn:
            # ë°ì´í„° ì¤€ë¹„
            word_df = df_daily[df_daily['word'] == keyword].copy()
            if word_df.empty:
                st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                word_df['date'] = pd.to_datetime(word_df['date'])
                word_df = word_df.set_index('date').sort_index()
                
                # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
                min_date = word_df.index.min() - datetime.timedelta(days=5)
                max_date = word_df.index.max() + datetime.timedelta(days=5)
                
                with st.spinner(f"{ticker} ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                    try:
                        stock_df = yf.download(ticker, start=min_date, end=max_date, progress=False)
                        
                        if stock_df.empty:
                            st.error("ì£¼ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í‹°ì»¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                        else:
                            # ë°ì´í„° ë³‘í•©
                            merged_df = stock_df.copy()
                            # MultiIndex ì»¬ëŸ¼ ì²˜ë¦¬ (yfinance ìµœì‹ ë²„ì „ ëŒ€ì‘)
                            if isinstance(merged_df.columns, pd.MultiIndex):
                                merged_df.columns = merged_df.columns.get_level_values(0)

                            merged_df['WordCount'] = word_df['count']
                            merged_df['WordCount'] = merged_df['WordCount'].fillna(0)
                            
                            # mplfinance ì°¨íŠ¸ ìƒì„±
                            mc = mpf.make_marketcolors(up='red', down='blue', inherit=True)
                            s  = mpf.make_mpf_style(marketcolors=mc, gridstyle=':', y_on_right=True)
                            
                            # ì¶”ê°€ í”Œë¡¯ (ë‹¨ì–´ ë¹ˆë„ ë§‰ëŒ€)
                            ap = mpf.make_addplot(merged_df['WordCount'], type='bar', panel=1, color='purple', ylabel='Mentions')
                            
                            # Figure ê°ì²´ ë°˜í™˜ë°›ê¸° (returnfig=True)
                            fig, axes = mpf.plot(
                                merged_df, 
                                type='candle', 
                                style=s, 
                                addplot=ap, 
                                volume=False, # ê±°ë˜ëŸ‰ ëŒ€ì‹  ë‹¨ì–´ ë¹ˆë„ ì‚¬ìš©í•˜ë¯€ë¡œ ë”
                                returnfig=True,
                                panel_ratios=(2,1), # ìƒë‹¨ 2 : í•˜ë‹¨ 1 ë¹„ìœ¨
                                title=f"{ticker} Price vs '{keyword}' Sentiment",
                                figsize=(10, 8)
                            )
                            
                            # Streamlitì— í‘œì‹œ
                            st.pyplot(fig)
                            
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    with tab2:
        st.subheader("ìˆ˜ì§‘ëœ ë°ì´í„° í™•ì¸")
        if 'df_posts' in st.session_state:
            st.dataframe(st.session_state['df_posts'])
        else:
            st.dataframe(df_daily)

else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")