import time
import random
import re
from typing import List, Set, Optional
import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st
import plotly.express as px

# -----------------------------
# ì„¤ì • ë° ê¸°ì´ˆ í•¨ìˆ˜
# -----------------------------
try:
    from konlpy.tag import Okt
except ImportError:
    st.error("KoNLPy ë¯¸ì„¤ì¹˜")

DEFAULT_STOPWORDS = {
    "ê·¸ëƒ¥", "ê·¼ë°", "ê·¸ë¦¬ê³ ", "ì¢€", "ì´ê±°", "ì§„ì§œ", "ì¡´ë‚˜", "ì‹œë°œ", "ë³‘ì‹ ", 
    "ê°œì¶”", "ë¹„ì¶”", "ìƒê°", "ì‚¬ëŒ", "ì§€ê¸ˆ", "ì£¼ì‹", "ë§¤ìˆ˜", "ë§¤ë„", "ì˜¤ëŠ˜"
}

@st.cache_resource
def get_tokenizer():
    try:
        return Okt()
    except:
        return None

def tokenize(text):
    try:
        okt = get_tokenizer()
        nouns = okt.nouns(re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text))
        return [n for n in nouns if len(n) >= 2 and n not in DEFAULT_STOPWORDS]
    except:
        return text.split()

# -----------------------------
# V4: ì§„ë‹¨ ê¸°ëŠ¥ì„ í¬í•¨í•œ í¬ë¡¤ëŸ¬
# -----------------------------
def crawl_debug(gallery_id, gallery_type, start_page, end_page):
    # 1. í—¤ë” ì„¤ì • (ìµœëŒ€í•œ ì‚¬ëŒì²˜ëŸ¼)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://gall.dcinside.com/",
        "Connection": "keep-alive"
    }

    session = requests.Session()
    session.headers.update(headers)
    
    # 2. ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸í•´ì„œ ì¿ í‚¤ íšë“ (ì°¨ë‹¨ ìš°íšŒ ì‹œë„)
    try:
        session.get("https://gall.dcinside.com")
    except:
        pass

    rows = []
    
    # URL ì„¤ì •
    base_url = "https://gall.dcinside.com"
    if gallery_type == "minor":
        list_base = f"{base_url}/mgallery/board/lists/"
        view_base = f"{base_url}/mgallery/board/view/"
    elif gallery_type == "mini":
        list_base = f"{base_url}/mini/board/lists/"
        view_base = f"{base_url}/mini/board/view/"
    else:
        list_base = f"{base_url}/board/lists/"
        view_base = f"{base_url}/board/view/"

    progress = st.progress(0)
    
    # ë””ë²„ê·¸ìš© ë¡œê·¸
    debug_log = []

    for idx, page in enumerate(range(start_page, end_page + 1)):
        params = {'id': gallery_id, 'page': page}
        
        try:
            res = session.get(list_base, params=params, timeout=10)
            
            # ë””ë²„ê·¸: ì‘ë‹µ ìƒíƒœ ì €ì¥
            if idx == 0:
                debug_log.append(f"ì‘ë‹µ ì½”ë“œ: {res.status_code}")
                debug_log.append(f"URL: {res.url}")
                debug_log.append(f"HTML ì•ë¶€ë¶„(500ì): {res.text[:500]}")
            
            soup = BeautifulSoup(res.text, "html.parser")
            
            # ê²Œì‹œê¸€ í–‰ ì°¾ê¸°
            trs = soup.select("tr.ub-content.us-post")
            if not trs: trs = soup.select("tr.ub-content")

            if not trs:
                debug_log.append(f"{page}í˜ì´ì§€: ê²Œì‹œê¸€ í…Œì´ë¸”(tr)ì„ ì°¾ì§€ ëª»í•¨.")
                continue

            for tr in trs:
                if "ê³µì§€" in tr.get_text(): continue
                
                a = tr.select_one("a.ub-word")
                if not a: continue
                
                title = a.get_text(strip=True)
                link = a.get("href")
                
                if not link: continue
                match = re.search(r'no=([0-9]+)', link)
                if match:
                    post_url = f"{view_base}?id={gallery_id}&no={match.group(1)}"
                    
                    # ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ (ìƒëµ ê°€ëŠ¥)
                    content = ""
                    try:
                        time.sleep(random.uniform(0.1, 0.5))
                        pr = session.get(post_url, timeout=5)
                        ps = BeautifulSoup(pr.text, "html.parser")
                        cd = ps.select_one("div.write_div")
                        if cd: content = cd.get_text(separator=" ", strip=True)
                    except: pass
                    
                    # ë‚ ì§œ (ê°„ë‹¨ì²˜ë¦¬)
                    import datetime
                    rows.append({
                        "date": datetime.datetime.now().date(), # ë‚ ì§œ íŒŒì‹± ë³µì¡í•´ì„œ ì¼ë‹¨ ì˜¤ëŠ˜ë¡œ í†µì¼
                        "title": title,
                        "content": content
                    })
                    
        except Exception as e:
            debug_log.append(f"ì—ëŸ¬ ë°œìƒ: {e}")
            
        progress.progress((idx+1)/(end_page-start_page+1))

    return pd.DataFrame(rows), debug_log

# -----------------------------
# ë©”ì¸ UI
# -----------------------------
def main():
    st.set_page_config("ë””ì”¨ ë¶„ì„ê¸° V4 (ì§„ë‹¨ëª¨ë“œ)")
    st.title("ğŸ•µï¸ ë””ì”¨ ë¶„ì„ê¸° V4 (ì°¨ë‹¨ ì§„ë‹¨)")
    
    with st.sidebar:
        gid = st.text_input("ê°¤ëŸ¬ë¦¬ ID", "stockus")
        gtype = st.radio("ì¢…ë¥˜", ["minor", "major", "mini"])
        if st.button("ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("ì ‘ì† ì‹œë„ ì¤‘..."):
                df, logs = crawl_debug(gid, gtype, 1, 2)
                st.session_state['logs'] = logs
                st.session_state['df'] = df

    # ê²°ê³¼ í™”ë©´
    if 'df' in st.session_state:
        df = st.session_state['df']
        logs = st.session_state.get('logs', [])

        if df.empty:
            st.error("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨!")
            st.warning("ë””ì‹œì¸ì‚¬ì´ë“œê°€ ì ‘ì†ì„ ì°¨ë‹¨í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì•„ë˜ ë””ë²„ê·¸ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            with st.expander("ğŸ› ï¸ ë””ë²„ê·¸: ì™œ ì‹¤íŒ¨í–ˆë‚˜ìš”?", expanded=True):
                for log in logs:
                    st.text(log)
                    st.markdown("---")
                
                st.markdown("""
                ### ğŸ” ë¶„ì„ ê²°ê³¼
                1. **HTMLì— 'location.replace' ë“±ì´ ë³´ì¸ë‹¤ë©´?** -> ì°¨ë‹¨ë¨ (Bot Detection)
                2. **HTMLì´ ì •ìƒì ì¸ë° ë°ì´í„°ê°€ ì—†ë‹¤ë©´?** -> ê°¤ëŸ¬ë¦¬ IDë‚˜ ì¢…ë¥˜(ë§ˆì´ë„ˆ/ì •ì‹) ì„¤ì • ì‹¤ìˆ˜
                3. **ì‘ë‹µ ì½”ë“œê°€ 403/404ë¼ë©´?** -> IP ì°¨ë‹¨
                
                **ğŸ‘‰ í•´ê²°ì±…: ì´ ì½”ë“œë¥¼ Streamlit Cloudê°€ ì•„ë‹Œ 'ë‚´ ì»´í“¨í„°'ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.**
                """)
        else:
            st.success(f"âœ… ì„±ê³µ! {len(df)}ê°œ ìˆ˜ì§‘ë¨")
            st.dataframe(df)
            
            # ê°„ë‹¨ ë¶„ì„
            all_text = " ".join(df['title'] + " " + df['content'])
            words = tokenize(all_text)
            st.write(pd.Series(words).value_counts().head(20))

if __name__ == "__main__":
    main()